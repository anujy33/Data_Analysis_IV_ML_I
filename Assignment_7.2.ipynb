{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. What are the three stages to build the hypotheses or model in machine learning?\n",
    "2. What is the standard approach to supervised learning?\n",
    "3. What is Training set and Test set?\n",
    "4. What is the general principle of an ensemble method and what is bagging and\n",
    "boosting in ensemble method?\n",
    "5. How can you avoid overfitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer1 :- THree stages to uild the hypotheses or model in machine learning are:\n",
    " a) Model Build\n",
    " b) Model Testing\n",
    " c) Applying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer2 : - Supervised learning algorithms are trained using labeled explamples, such as an input where the desired output is\n",
    " is known. The learning alorith receives a set of inputs along with the corresponiding correct output, and the algoritm receives a set of input along with the corresponding correct output, and the algorithm learns by comparing its actual output with correct outputs to find error. It then modifies the model accordingly through methods like classification, regression, prediction and gradient boosting. Supervised learning uses patterns to predict the values of the label on additional unlabeled data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer3:\n",
    "        \n",
    "        Training Set :\n",
    "             In machine learning, a training set is a dataset used to train a model. In training the model, specific features are picked out from traning set. These features are then incorporated into the model. \n",
    "             \n",
    "         Test Set :\n",
    "             The test set is a dataset used ti measure how well the model performs at making predictions on that test set. If the prediction score for the test set are unreasonable, we will have to make some adjustments to our model and try again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer4: - General principle of ensemble model is to combine multiple ‘individual’ (diverse) models together and delivers superior prediction power.Basically, an ensemble is a supervised learning technique for combining multiple weak learners/ models to produce a strong learner.\n",
    "\n",
    "Bagging: -Bagging is an ensemble method. First, we create random samples of the training data set (sub sets of training data set). Then, we build a classifier for each sample. Finally, results of these multiple classifiers are combined using average or majority voting. Bagging helps to reduce the variance error.\n",
    "\n",
    "Boosting :- Boosting provides sequential learning of the predictors. The first predictor is learned on the whole data set, while the following are learnt on the training set based on the performance of the previous one. It starts by classifying original data set and giving equal weights to each observation. If classes are predicted incorrectly using the first learner, then it gives higher weight to the missed classified observation. Being an iterative process, it continues to add classifier learner until a limit is reached in the number of models or accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 5:- Overfitting happens when a model learns the details and noise in the training data to the extent that it negatively impacts the performance of the model oe new data. This means that the noise or random fluctutaions in the training data is picket up and learned as concepts by the model. To get the sweet spot we can look at the performance of machine learning algorithm over time as it is learning a training data. We can plot both skill on the training data and the skill on test dadtaset, which has been held back from the training process.\n",
    "Overtime as the algoritm learns, the error for the model on training data goes down and so does the error on the test dataset. If we train for too long, the performance on the training dataset may continue to decrease because the model is overfitting and learning irrelevant details and noises in the training dataset. At the same time the error for the test dataset starts to rise again as the model 's ability to generalize decreases. \n",
    "The sweet point is the point just before the error on the test dataset starts to increase where the model has good skill on both the training dataset and the unseen test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
